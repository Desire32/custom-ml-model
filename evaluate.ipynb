{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH9x9_XwBfgG"
      },
      "outputs": [],
      "source": [
        "# %pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer, # language models\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer, # fine-tuning\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling, # part of pipeline responsible for assembling\n",
        "    BitsAndBytesConfig, # BitsAndBytes for quantum compression, less data size for models, yet not losing in speed at all\n",
        "    )\n",
        "\n",
        "import torch # pyTorch\n",
        "import safetensors.torch\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model # (Parameter Efficient Fine Tuning) -> we take LoRA only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exPJHbi9Bj8m"
      },
      "outputs": [],
      "source": [
        "def import_model(model_name):\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16, ## gpu support\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=BitsAndBytesConfig(load_in_8bit=True) # bitsAndBytes\n",
        "    )\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  tokenizer.pad_token = tokenizer.eos_token # in case some custom models dont have pad_token by default\n",
        "  return model, tokenizer\n",
        "\n",
        "models = [\"deepseek-ai/deepseek-llm-7b-base\", \"mistralai/Mistral-7B-Instruct-v0.3\", \"meta-llama/Llama-3.1-8B-Instruct\", \"microsoft/phi-2\"]\n",
        "model, tokenizer = import_model[models[2]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouPF5G0xCV1M"
      },
      "outputs": [],
      "source": [
        "def lora_training(model):\n",
        "  model = prepare_model_for_kbit_training(model)\n",
        "  lora_config = LoraConfig(\n",
        "    r=8, # a rank, the bigger the rank, more accuracy we get, but becomes slower\n",
        "    lora_alpha=42, # an influence of LoRA on a model\n",
        "    target_modules=[\"q_proj\", \"v_proj\"], # there are modules we touch to change, q_proj = query, v_proj = value\n",
        "    lora_dropout=0.05, # in order to avoid overtraining\n",
        "    bias=\"none\", # means do not touch bias\n",
        "    task_type=\"CAUSAL_LM\" # model wise\n",
        "  )\n",
        "  model = get_peft_model(model, lora_config)\n",
        "  model.gradient_checkpointing_enable()\n",
        "  return model\n",
        "\n",
        "model = lora_training(model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHSdmKxVIst3"
      },
      "outputs": [],
      "source": [
        "model.load_adapter(\"write_path_to_adapter_here\", adapter_name=\"custom\")\n",
        "model.set_adapter(\"custom\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ7oFTuEDduy"
      },
      "outputs": [],
      "source": [
        "def gen(question, model, tokenizer):\n",
        "    prompt = f\"Question: {question}\\nAnswer:\" # <-------- customize your prompt here\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=500,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True, ## variety, turn off for now\n",
        "        top_p=0.95,\n",
        "        temperature=0.7, ## temp\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = answer.split(\"Answer:\")[-1].strip()\n",
        "    return answer\n",
        "\n",
        "gen(\"write question here\", model, tokenizer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyO9gVPhJWlsrUQRA6N7qhdM",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
