{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKHwIEJB+ISRJKRx4BeuKO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_oFZXYhR4c9"
      },
      "outputs": [],
      "source": [
        "# !pip install faiss-cpu\n",
        "# !pip install gensim\n",
        "# !pip install pybind11\n",
        "# !pip install numpy<2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss # our agent to find connection between vectors\n",
        "from gensim.models import Word2Vec # conversion words into vectors\n",
        "import numpy as np # math\n",
        "\n",
        "import nltk # to convert text into some more presentable way\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "id": "bqSDlHy6VdwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def file_convert(file_name):\n",
        "  with open(file_name, \"r\", encoding=\"utf-8\") as file: ## reading a file and storing it into one variable\n",
        "    f = file.read()\n",
        "\n",
        "  sentenses = sent_tokenize(f) # our english teacher\n",
        "  tokenized_sentenses = [word_tokenize(sent) for sent in sentenses] # structure a text\n",
        "\n",
        "  model = Word2Vec(tokenized_sentenses, vector_size=100, window=5, min_count=2, sg=1) # convertation to vectors\n",
        "  document_vectors = [] # TODO\n",
        "\n",
        "  for sentence in tokenized_sentenses:\n",
        "      sentence_vector = np.mean([model.wv[word] for word in sentence if word in model.wv], axis=0)\n",
        "      if np.isnan(sentence_vector).any():\n",
        "          sentence_vector = np.zeros(model.vector_size)\n",
        "      document_vectors.append(sentence_vector)\n",
        "  return document_vectors, model, tokenized_sentenses\n"
      ],
      "metadata": {
        "id": "VazXjzjgeHlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_vec, model, dataset = file_convert(\"church_text\")\n",
        "print(doc_vec[:3])\n",
        "print(np.linalg.norm(doc_vec, axis=1))\n",
        "\n",
        "# from sklearn.preprocessing import normalize -> 4 decimal difference (less than 1%)\n",
        "# norm_vec = normalize(doc_vec, axis=1)\n",
        "# print(norm_vec[:1])"
      ],
      "metadata": {
        "id": "jnOTPn-ZWnpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def faiss_search(document, model): # now we initialize a search on a given field\n",
        "  document_vectors = np.array(document).astype(\"float32\")\n",
        "\n",
        "  index = faiss.IndexFlatIP(model.vector_size)\n",
        "  index.add(document_vectors)  # add vectors to FAISS\n",
        "  return index, model"
      ],
      "metadata": {
        "id": "UU-IgSoAdlvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_index, faiss_model = faiss_search(doc_vec ,model)"
      ],
      "metadata": {
        "id": "ivqJ_64WiQjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### models part\n",
        "import torch # pytorch\n",
        "import pandas as pd # datasets\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ],
      "metadata": {
        "id": "wuAcB-0M12uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phi = \"microsoft/phi-2\"\n",
        "phi_model = AutoModelForCausalLM.from_pretrained(\n",
        "    phi,\n",
        "    torch_dtype=torch.float16, ## gpu support\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True ## fix api issues\n",
        "    )\n",
        "phi_tokenizer = AutoTokenizer.from_pretrained(phi)"
      ],
      "metadata": {
        "id": "SSLh7Abe3k6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"church robot\"\n",
        "\n",
        "sntc_question = sent_tokenize(question)\n",
        "tokenized_question = [word_tokenize(sent) for sent in sntc_question]\n",
        "\n",
        "# vectorization with numpy\n",
        "words = [model.wv[word] for word in tokenized_question[0] if word in model.wv]\n",
        "\n",
        "if not words:\n",
        "    question_vector = np.zeros(model.vector_size)\n",
        "else:\n",
        "    question_vector = np.mean(words, axis=0)\n",
        "\n",
        "query = question_vector.reshape(1, -1).astype(\"float32\")\n",
        "query[:1]"
      ],
      "metadata": {
        "id": "gWDM5ETBjdn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# faiss search\n",
        "distances, indices = faiss_index.search(query, k=3)\n",
        "\n",
        "threshold = 0.2 # ограничение по \"релевантности\" запроса, насколько соотвествует ожиданиям, сортировка мусора\n",
        "\n",
        "if distances[0][0] < threshold:\n",
        "  print(\"trash values\")\n",
        "else:\n",
        "  for idx in indices[0]:\n",
        "    print(dataset[idx])\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "cRpfW9r3kV7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2YthK0-mlOPi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}