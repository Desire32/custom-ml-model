{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPWCpcpsJccLLLNfjpd0JUc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_oFZXYhR4c9"
      },
      "outputs": [],
      "source": [
        "# !pip install faiss-cpu\n",
        "# !pip install gensim\n",
        "# !pip install pybind11\n",
        "# !pip install numpy<2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss # chaining vectors into one meaningful one\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "import nltk # to convert text into some more presentable way\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "id": "bqSDlHy6VdwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentenses_convert(file_name):\n",
        "  with open(file_name, \"r\", encoding=\"utf-8\") as file: ## reading a file and storing it into one variable\n",
        "    f = file.read()\n",
        "\n",
        "  sentenses = sent_tokenize(f)\n",
        "  tokenized_sentenses = [word_tokenize(sent) for sent in sentenses] # structure a text\n",
        "\n",
        "  model = Word2Vec(tokenized_sentenses, vector_size=100, window=5, min_count=2, sg=1) # convertation to vectors\n",
        "  document_vectors = [] # TODO\n",
        "  for sentence in tokenized_sentenses:\n",
        "      sentence_vector = np.mean([model.wv[word] for word in sentence if word in model.wv], axis=0)\n",
        "      if np.isnan(sentence_vector).any():\n",
        "          sentence_vector = np.zeros(model.vector_size)\n",
        "      document_vectors.append(sentence_vector)\n",
        "  return document_vectors, model\n"
      ],
      "metadata": {
        "id": "VazXjzjgeHlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_vec, model = sentenses_convert(\"church_text\")"
      ],
      "metadata": {
        "id": "jnOTPn-ZWnpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def faiss_convertion(document, model): # now we collect our vectors into matrices\n",
        "\n",
        "  document_vectors = np.array(document).astype(\"float32\")\n",
        "\n",
        "  index = faiss.IndexFlatL2(model.vector_size)\n",
        "  index.add(document_vectors)  # add vectors to FAISS\n",
        "  return index, model"
      ],
      "metadata": {
        "id": "UU-IgSoAdlvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_index, faiss_model = faiss_convertion(doc_vec,model)"
      ],
      "metadata": {
        "id": "ivqJ_64WiQjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### models part\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "wuAcB-0M12uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phi = \"microsoft/phi-2\"\n",
        "phi_model = AutoModelForCausalLM.from_pretrained(\n",
        "    phi,\n",
        "    torch_dtype=torch.float16, ## gpu support\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True ## fix api issues\n",
        "    )\n",
        "phi_tokenizer = AutoTokenizer.from_pretrained(phi)"
      ],
      "metadata": {
        "id": "SSLh7Abe3k6P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}