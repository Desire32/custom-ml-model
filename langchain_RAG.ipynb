{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain_community faiss-cpu langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from langchain_community.vectorstores import FAISS # semantic search\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore # dict doc store\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings # embedding model\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def file_read(file_name):\n",
    "  with open(file_name, 'r') as file:\n",
    "    file = file.read()\n",
    "  dataset = Dataset.from_dict({\"text\": file.split(\"\\n\\n\")})\n",
    "  return dataset\n",
    "\n",
    "dataset = file_read(\"./cache/church_text\") # file\n",
    "text = dataset[\"text\"] # key\n",
    "embedder = HuggingFaceEmbeddings()\n",
    "sample_key = embedder.embed_query(text[0])\n",
    "\n",
    "## bart faiss\n",
    "\n",
    "index = faiss.IndexFlatL2(len(sample_key)) # первая операция для того чтобы указать faiss размерность входящих эмбеддингов\n",
    "\n",
    "vectors = [embedder.embed_query(t) for t in text] # загружаем уже весь датасет\n",
    "index.add(np.array(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "vector_storage = FAISS( # works with semantic search & rag pipelines\n",
    "    embedding_function=HuggingFaceEmbeddings(), # convert text to vectors\n",
    "    index=index, # our loaded vectors\n",
    "    docstore= InMemoryDocstore(), # dict in memory\n",
    "    index_to_docstore_id={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def import_model(model_name):\n",
    "  model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16, ## gpu support\n",
    "    device_map=\"auto\",\n",
    "    # quantization_config=BitsAndBytesConfig(load_in_8bit=True) # bitsAndBytes\n",
    "    )\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  tokenizer.pad_token = tokenizer.eos_token # in case some custom models dont have pad_token by default\n",
    "  return model, tokenizer\n",
    "\n",
    "models = [\"deepseek-ai/deepseek-llm-7b-base\", \"mistralai/Mistral-7B-Instruct-v0.3\", \"meta-llama/Llama-3.1-8B-Instruct\", \"microsoft/phi-2\"]\n",
    "model, tokenizer = import_model(models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def gen(question,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        embedder,\n",
    "        top_k=3 # somewhat simmilar with batch_size, we take range of simmilar topics from vectors\n",
    "        ):\n",
    "    # vector store search\n",
    "    embed = embedder.embed_query(question) # load our question into vector store (rag)\n",
    "    D, I = index.search(np.array([embed]), top_k) # top_k simillar answers\n",
    "    retrieved_texts = [dataset[\"text\"][i] for i in I[0]] # take what we found\n",
    "    context = \"\\n\".join(retrieved_texts) # join them together\n",
    "\n",
    "    prompt = f\"Context:{context}, Question: {question}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=500,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=False, ## variety, turn off for now\n",
    "        top_p=0.95,\n",
    "        temperature=0.7, ## temp\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = answer.split(\"Answer:\")[-1].strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "gen(\"larnaka church\", model, tokenizer, embedder)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
