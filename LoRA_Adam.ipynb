{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOsxxjYB/74ODPsGgEWmvxx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3I_ill3FBZmh"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer, # language models\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer, # fine-tuning\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling, # part of pipeline responsible for assembling\n",
        "    BitsAndBytesConfig # BitsAndBytes for quantum compression, less data size for models, yet not losing in speed at all\n",
        "    )\n",
        "\n",
        "import torch # pyTorch\n",
        "\n",
        "from datasets import Dataset # converting text file in dataset (increases file reading speed)\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model # (Parameter Efficient Fine Tuning) -> we take LoRA only\n",
        "import torch.optim.adam # Adaptive Moment Estimation (Adam)\n",
        "\n",
        "# !pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def import_model(model_name):\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16, ## gpu support\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=BitsAndBytesConfig(load_in_8bit=True) # bitsAndBytes\n",
        "    )\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  tokenizer.pad_token = tokenizer.eos_token # in case some custom models dont have pad_token by default\n",
        "  return model, tokenizer\n",
        "\n",
        "models = [\"microsoft/phi-2\", \"mistralai/Mistral-7B-Instruct-v0.3\"]\n",
        "# phi_model, phi_tokenizer = import_model(models[0])\n",
        "mistral_model, mistral_tokenizer = import_model(models[1])"
      ],
      "metadata": {
        "id": "fP8c9eEhHSgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def file_read(file_name):\n",
        "  with open(file_name, 'r') as file:\n",
        "    file = file.read()\n",
        "    # dataset = [sentences[i:i+40] for i in range(0, len(sentences), 40)] # aint work, there's going to be an error, to large dataset, need to use Dataset\n",
        "  dataset = Dataset.from_dict({\"text\": file.split(\"\\n\\n\")})\n",
        "  return dataset\n",
        "\n",
        "dataset = file_read(\"smaller_size\")\n",
        "\n",
        "def make_token_func(tokenizer, max_length=512): # a constructor for different tokenizers\n",
        "    def token_func(example):\n",
        "        return tokenizer(\n",
        "            example[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "        )\n",
        "    return token_func\n",
        "\n",
        "token_func = make_token_func(mistral_tokenizer)\n",
        "token_dataset = dataset.map(token_func, batched=True)"
      ],
      "metadata": {
        "id": "0vQyzK3HDTX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lora_training(model):\n",
        "  model = prepare_model_for_kbit_training(model)\n",
        "  lora_config = LoraConfig(\n",
        "    r=8, # a rank, the bigger the rank, more accuracy we get, but becomes slower\n",
        "    lora_alpha=42, # an influence of LoRA on a model\n",
        "    target_modules=[\"q_proj\", \"v_proj\"], # there are modules we touch to change, q_proj = query, v_proj = value\n",
        "    lora_dropout=0.05, # in order to avoid overtraining\n",
        "    bias=\"none\", # means do not touch bias\n",
        "    task_type=\"CAUSAL_LM\" # model wise\n",
        "  )\n",
        "  model = get_peft_model(model, lora_config)\n",
        "  model.gradient_checkpointing_enable()\n",
        "  return model\n",
        "\n",
        "mistral_model = lora_training(mistral_model)\n"
      ],
      "metadata": {
        "id": "sdNVLVJZFh1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_train(model, tokenizer, dataset):\n",
        "  data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        "  )\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir=f\"./{model.__class__.__name__}-continued-pretrain\",\n",
        "      overwrite_output_dir=True,\n",
        "      max_steps=1000,\n",
        "      per_device_train_batch_size=1,\n",
        "      save_steps=250,\n",
        "      save_total_limit=1,\n",
        "      prediction_loss_only=True,\n",
        "      fp16=False,  # Disable fp16 for T4\n",
        "      learning_rate=5e-6,\n",
        "      logging_steps=50,\n",
        "  )\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=dataset,\n",
        "      tokenizer=tokenizer,\n",
        "      data_collator=data_collator,\n",
        "  )\n",
        "  trainer.train()\n",
        "\n",
        "model_train(mistral_model, mistral_tokenizer, token_dataset)"
      ],
      "metadata": {
        "id": "nnUfaf-GJ823"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}