{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3I_ill3FBZmh"
      },
      "outputs": [],
      "source": [
        "# %pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer, # language models\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer, # fine-tuning\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling, # part of pipeline responsible for assembling\n",
        "    BitsAndBytesConfig, # BitsAndBytes for quantum compression, less data size for models, yet not losing in speed at all\n",
        "    )\n",
        "\n",
        "import torch # pyTorch\n",
        "import safetensors.torch\n",
        "\n",
        "from datasets import Dataset # converting text file in dataset (increases file reading speed)\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model # (Parameter Efficient Fine Tuning) -> we take LoRA only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fP8c9eEhHSgG"
      },
      "outputs": [],
      "source": [
        "def import_model(model_name):\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16, ## gpu support\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=BitsAndBytesConfig(load_in_8bit=True) # bitsAndBytes\n",
        "    )\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  tokenizer.pad_token = tokenizer.eos_token # in case some custom models dont have pad_token by default\n",
        "  return model, tokenizer\n",
        "\n",
        "models = [\"deepseek-ai/deepseek-llm-7b-base\", \"mistralai/Mistral-7B-Instruct-v0.3\", \"meta-llama/Llama-3.1-8B-Instruct\", \"microsoft/phi-2\"]\n",
        "model, tokenizer = import_model(models[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vQyzK3HDTX_"
      },
      "outputs": [],
      "source": [
        "def file_read(file_name):\n",
        "  with open(file_name, 'r') as file:\n",
        "    file = file.read()\n",
        "  dataset = Dataset.from_dict({\"text\": file.split(\"\\n\\n\")})\n",
        "  return dataset\n",
        "\n",
        "dataset = file_read(\"church_text\")\n",
        "\n",
        "def make_token_func(tokenizer, max_length=512): # a constructor for different tokenizers\n",
        "    def token_func(example):\n",
        "        return tokenizer(\n",
        "            example[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "        )\n",
        "    return token_func\n",
        "\n",
        "token_func = make_token_func(tokenizer)\n",
        "token_dataset = dataset.map(token_func, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdNVLVJZFh1T"
      },
      "outputs": [],
      "source": [
        "def lora_training(model):\n",
        "  model = prepare_model_for_kbit_training(model)\n",
        "  lora_config = LoraConfig(\n",
        "    r=8, # a rank, the bigger the rank, more accuracy we get, but becomes slower\n",
        "    lora_alpha=42, # an influence of LoRA on a model\n",
        "    target_modules=[\"q_proj\", \"v_proj\"], # there are modules we touch to change, q_proj = query, v_proj = value\n",
        "    lora_dropout=0.05, # in order to avoid overtraining\n",
        "    bias=\"none\", # means do not touch bias\n",
        "    task_type=\"CAUSAL_LM\" # model wise\n",
        "  )\n",
        "  model = get_peft_model(model, lora_config)\n",
        "  model.gradient_checkpointing_enable()\n",
        "  return model\n",
        "\n",
        "model = lora_training(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnUfaf-GJ823"
      },
      "outputs": [],
      "source": [
        "def model_train(model, tokenizer, dataset):\n",
        "  data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        "  )\n",
        "\n",
        "  # adam is working under the hood by default\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir=f\"./pretrain-max-steps\",\n",
        "      overwrite_output_dir=True,\n",
        "      max_steps=1000,\n",
        "      per_device_train_batch_size=10, # количество рассмотренных обьектов за один раз -> усреднение -> лучшая точность\n",
        "      save_steps=50,\n",
        "      save_total_limit=1,\n",
        "      prediction_loss_only=True,\n",
        "      fp16=True,\n",
        "      learning_rate=5e-6,\n",
        "      logging_steps=20,\n",
        "  )\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=dataset,\n",
        "      tokenizer=tokenizer,\n",
        "      data_collator=data_collator,\n",
        "  )\n",
        "  trainer.train()\n",
        "\n",
        "model_train(model, tokenizer, token_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSL729gBSQWT"
      },
      "outputs": [],
      "source": [
        "def lora_adapters_attach(model, checkpoint):\n",
        "  adapters_weights = safetensors.torch.load_file(f\"./llama-continued-pretrain-max_steps{1000}/checkpoint-{checkpoint}/adapter_model.safetensors\")\n",
        "  model.load_state_dict(adapters_weights, strict=False)\n",
        "  return model\n",
        "\n",
        "model = lora_adapters_attach(model, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwUuHaMeTnEB"
      },
      "outputs": [],
      "source": [
        "def gen(question, model, tokenizer):\n",
        "    prompt = f\"Question: {question}\\nAnswer:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=500,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=False, ## variety, turn off for now\n",
        "        top_p=0.95,\n",
        "        temperature=0.7, ## temp\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = answer.split(\"Answer:\")[-1].strip()\n",
        "    return answer\n",
        "\n",
        "gen(\"prompt\", model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mt3r7pmUQmP"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "def save_trained_model(model, checkpoint):\n",
        "  file = shutil.make_archive(f\"{model.__class__.__name__}-checkpoint\", 'zip', f\"./pretrain-max_steps/checkpoint-{checkpoint}/\")\n",
        "  files.download(file)\n",
        "\n",
        "save_trained_model(model, 1000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMuVKqOC0C1D/RrOpFGPW8n",
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
