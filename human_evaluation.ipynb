{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO9gVPhJWlsrUQRA6N7qhdM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH9x9_XwBfgG"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer, # language models\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer, # fine-tuning\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling, # part of pipeline responsible for assembling\n",
        "    BitsAndBytesConfig, # BitsAndBytes for quantum compression, less data size for models, yet not losing in speed at all\n",
        "    )\n",
        "\n",
        "import torch # pyTorch\n",
        "import safetensors.torch\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model # (Parameter Efficient Fine Tuning) -> we take LoRA only\n",
        "\n",
        "# !pip install -U bitsandbytes\n",
        "# !pip install --upgrade transformers\n",
        "# !unzip deepseek-clear.zip -d deepseek/\n",
        "# !unzip mistral500steps.zip -d mistral/\n",
        "# !unzip llama-1000steps.zip -d llama/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def import_model(model_name):\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16, ## gpu support\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=BitsAndBytesConfig(load_in_8bit=True) # bitsAndBytes\n",
        "    )\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  tokenizer.pad_token = tokenizer.eos_token # in case some custom models dont have pad_token by default\n",
        "  return model, tokenizer\n",
        "\n",
        "models = [\"deepseek-ai/deepseek-llm-7b-base\", \"mistralai/Mistral-7B-Instruct-v0.3\", \"meta-llama/Llama-3.1-8B-Instruct\"]\n",
        "# deepseek_model, deepseek_tokenizer = import_model(models[0])\n",
        "# mistral_model, mistral_tokenizer = import_model(models[1])\n",
        "# llama_model, llama_tokenizer = import_model(models[2])"
      ],
      "metadata": {
        "id": "exPJHbi9Bj8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lora_training(model):\n",
        "  model = prepare_model_for_kbit_training(model)\n",
        "  lora_config = LoraConfig(\n",
        "    r=8, # a rank, the bigger the rank, more accuracy we get, but becomes slower\n",
        "    lora_alpha=42, # an influence of LoRA on a model\n",
        "    target_modules=[\"q_proj\", \"v_proj\"], # there are modules we touch to change, q_proj = query, v_proj = value\n",
        "    lora_dropout=0.05, # in order to avoid overtraining\n",
        "    bias=\"none\", # means do not touch bias\n",
        "    task_type=\"CAUSAL_LM\" # model wise\n",
        "  )\n",
        "  model = get_peft_model(model, lora_config)\n",
        "  model.gradient_checkpointing_enable()\n",
        "  return model\n",
        "\n",
        "# deepseek_model = lora_training(deepseek_model)\n",
        "# mistral_model = lora_training(mistral_model)\n",
        "# llama_model = lora_training(llama_model)"
      ],
      "metadata": {
        "id": "ouPF5G0xCV1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mistral_model.load_adapter(\"./mistral\", adapter_name=\"custom\")\n",
        "# mistral_model.set_adapter(\"custom\")\n",
        "\n",
        "# deepseek_model.load_adapter(\"./deepseek\", adapter_name=\"custom\")\n",
        "# deepseek_model.set_adapter(\"custom\")\n",
        "\n",
        "# llama_model.load_adapter(\"./llama\", adapter_name=\"custom\")\n",
        "# llama_model.set_adapter(\"custom\")\n"
      ],
      "metadata": {
        "id": "pHSdmKxVIst3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen(question, model, tokenizer):\n",
        "    prompt = f\"Question: {question}\\nAnswer:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=500,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True, ## variety, turn off for now\n",
        "        top_p=0.95,\n",
        "        temperature=0.7, ## temp\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = answer.split(\"Answer:\")[-1].strip()\n",
        "    return answer\n",
        "\n",
        "# gen(\"The church of Panagia Aggeloktisti\", deepseek_model, deepseek_tokenizer)\n",
        "# gen(\"The church of Panagia Aggeloktisti\", mistral_model, mistral_tokenizer)\n",
        "# gen(\"The church of Panagia Aggeloktisti\", llama_model, llama_tokenizer)"
      ],
      "metadata": {
        "id": "nZ7oFTuEDduy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}