{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain_community faiss-cpu langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from langchain_community.vectorstores import FAISS # semantic search\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore # dict doc store\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings # embedding model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LangChainRAGPipeline:\n",
    "  def __init__(self, model_name, file_path):\n",
    "    self.model_name = model_name\n",
    "    self.file_path = file_path\n",
    "\n",
    "    self.model = None\n",
    "    self.tokenizer = None\n",
    "    self.dataset = None\n",
    "    self.index = None\n",
    "    self.vectorstore = None\n",
    "    self.embedder = HuggingFaceEmbeddings()\n",
    "\n",
    "    self.file_read()\n",
    "    self.build_index()\n",
    "    self.import_model()\n",
    "\n",
    "  def file_read(self):\n",
    "    with open(self.file_path, 'r') as file:\n",
    "        data = file.read()\n",
    "    self.dataset = Dataset.from_dict({\"text\": data.split(\"\\n\\n\")})\n",
    "\n",
    "  def build_index(self):\n",
    "    embedder = HuggingFaceEmbeddings()\n",
    "    text_data = self.dataset[\"text\"]\n",
    "    vectors = [self.embedder.embed_query(t) for t in text_data]\n",
    "\n",
    "    dim = len(vectors[0])\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(vectors))\n",
    "    self.index = index\n",
    "\n",
    "    self.vectorstore = FAISS( # works with semantic search & rag pipelines\n",
    "      embedding_function=HuggingFaceEmbeddings(), # convert text to vectors\n",
    "      index=self.index, # our loaded vectors\n",
    "      docstore= InMemoryDocstore(), # dict in memory\n",
    "      index_to_docstore_id={}\n",
    "    )\n",
    "\n",
    "  def import_model(self):\n",
    "    self.model = AutoModelForCausalLM.from_pretrained(\n",
    "      self.model_name,\n",
    "      torch_dtype=torch.float16, ## eat less resources\n",
    "      device_map=\"auto\",\n",
    "      # quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n",
    "    )\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "    self.tokenizer.pad_token = self.tokenizer.eos_token # in case some custom models dont have pad_token by default   \n",
    "\n",
    "  def get_model_and_tokenizer(self):\n",
    "        return self.model, self.tokenizer\n",
    "\n",
    "  def gen(self, question, top_k=3, max_length=500):\n",
    "    # vector store search\n",
    "    embed = self.embedder.embed_query(question) # load our question into vector store (rag)\n",
    "    D, I = self.index.search(np.array([embed]), top_k) # top_k simillar answers\n",
    "    retrieved_texts = [self.dataset[\"text\"][i] for i in I[0]] # take what we found\n",
    "    context = \"\\n\".join(retrieved_texts) # join them together\n",
    "\n",
    "    prompt = f\"Context:{context}, Question: {question}\\nAnswer:\"\n",
    "    inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "    outputs = self.model.generate(\n",
    "        **inputs,\n",
    "        max_length=500,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=False, ## variety, turn off for now\n",
    "        top_p=0.95,\n",
    "        temperature=0.7, ## temp\n",
    "        pad_token_id=self.tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = answer.split(\"Answer:\")[-1].strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "models = [\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"mistralai/Mistral-7B-Instruct-v0.3\", \"meta-llama/Llama-3.1-8B-Instruct\", \"microsoft/phi-2\"]\n",
    "files = \"./church_text\"\n",
    "\n",
    "pipeline = LangChainRAGPipeline(\n",
    "    model_name=models[0], # <-- choose a model here\n",
    "    file_path=files # <-- choose a dataset path here\n",
    ")\n",
    "\n",
    "model, tokenizer = pipeline.get_model_and_tokenizer()\n",
    "response = pipeline.gen(\"What can you tell me about Larnaka church?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
