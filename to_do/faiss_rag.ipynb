{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMRDN2FBsJPxj87GTOaO80C"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#For now, Faiss-RAG approach implemented partially, only for robots no convertion to dialog from vectors\n",
        "\n",
        "Libraries import, downgrade numpy to make libs work together\n"
      ],
      "metadata": {
        "id": "JyptN_ZsN8vp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_oFZXYhR4c9"
      },
      "outputs": [],
      "source": [
        "# !pip install faiss-cpu\n",
        "# !pip install gensim\n",
        "# !pip install pybind11\n",
        "# !pip install numpy<2\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "secret_value_0 = user_secrets.get_secret(\"HF_KEY\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(token=secret_value_0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss # our agent to find connection between vectors\n",
        "from gensim.models import Word2Vec # conversion words into vectors\n",
        "import numpy as np # math\n",
        "\n",
        "import nltk # to convert text into some more presentable way\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "id": "bqSDlHy6VdwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "mistral = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    mistral,\n",
        "    torch_dtype=torch.float16, ## gpu support\n",
        "    device_map=\"auto\",\n",
        "    )\n",
        "mistral_tokenizer = AutoTokenizer.from_pretrained(mistral)\n"
      ],
      "metadata": {
        "id": "VazXjzjgeHlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def file_convert(file_name):\n",
        "  with open(file_name, \"r\", encoding=\"utf-8\") as file: ## reading a file and storing it into one variable\n",
        "    f = file.read()\n",
        "\n",
        "  sentenses = sent_tokenize(f) # our english teacher\n",
        "  tokenized_sentenses = [word_tokenize(sent) for sent in sentenses] # structure a text\n",
        "\n",
        "  model = Word2Vec(tokenized_sentenses, vector_size=100, window=5, min_count=2, sg=1) # convertation to vectors\n",
        "  document_vectors = [] # TODO\n",
        "\n",
        "  for sentence in tokenized_sentenses:\n",
        "      sentence_vector = np.mean([model.wv[word] for word in sentence if word in model.wv], axis=0)\n",
        "      if np.isnan(sentence_vector).any():\n",
        "          sentence_vector = np.zeros(model.vector_size)\n",
        "      document_vectors.append(sentence_vector)\n",
        "  return document_vectors, model, tokenized_sentenses\n"
      ],
      "metadata": {
        "id": "jnOTPn-ZWnpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_vec, model, dataset = file_convert(\"/kaggle/input/church-s/church_text\")\n",
        "#print(doc_vec[:1])\n",
        "\n",
        "# from sklearn.preprocessing import normalize -> 4 decimal difference (less than 1%)\n",
        "# norm_vec = normalize(doc_vec, axis=1)\n",
        "# print(norm_vec[:1])\n",
        "\n",
        "## create a function to chunk words into 50-100 words, instead of just one, we can't change it directly in\n",
        "## the function file_convert because we need to build document vectors\n",
        "\n",
        "def chunk_tokenized_sentences(tokenized_sentences, chunk_size=100, stride=20):\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    flat_words = [word for sentence in tokenized_sentences for word in sentence]\n",
        "\n",
        "    for i in range(0, len(flat_words), chunk_size - stride):\n",
        "        chunk = flat_words[i:i + chunk_size]\n",
        "        chunks.append(\" \".join(chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "chunk = chunk_tokenized_sentences(dataset, chunk_size=50)"
      ],
      "metadata": {
        "id": "UU-IgSoAdlvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def faiss_search(document, model): # now we initialize a search on a given field\n",
        "  document_vectors = np.array(document).astype(\"float32\")\n",
        "\n",
        "  index = faiss.IndexFlatIP(model.vector_size)\n",
        "  index.add(document_vectors)  # add vectors to FAISS\n",
        "  return index, model"
      ],
      "metadata": {
        "id": "ivqJ_64WiQjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_index, faiss_model = faiss_search(doc_vec,model)"
      ],
      "metadata": {
        "id": "wuAcB-0M12uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Larnaka\"\n",
        "\n",
        "sntc_question = sent_tokenize(question)\n",
        "tokenized_question = [word_tokenize(sent) for sent in sntc_question]\n",
        "\n",
        "# vectorization with numpy\n",
        "words = [model.wv[word] for word in tokenized_question[0] if word in model.wv]\n",
        "\n",
        "if not words:\n",
        "    question_vector = np.zeros(model.vector_size)\n",
        "else:\n",
        "    question_vector = np.mean(words, axis=0)\n",
        "\n",
        "query = question_vector.reshape(1, -1).astype(\"float32\")\n",
        "query[:1]"
      ],
      "metadata": {
        "id": "SSLh7Abe3k6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# faiss search\n",
        "distances, indices = faiss_index.search(query, k=3)\n",
        "\n",
        "threshold = 0.3 # ограничение по \"релевантности\" запроса, насколько соотвествует ожиданиям, сортировка мусора\n",
        "\n",
        "valid_indices = indices[distances > threshold]\n",
        "valid_distances = distances[distances > threshold]\n",
        "\n",
        "# context = [dataset[i] for i in valid_indices]\n",
        "\n",
        "if len(valid_indices) == 0:\n",
        "    print(\"trash values\")\n",
        "else:\n",
        "    # print(f\"relevant info: {context}\")\n",
        "    print(f\"in range: {valid_distances}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "gWDM5ETBjdn-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}