{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPOEYEzix6OA5dfallLfKHB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Desire32/ml/blob/main/models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZCt5FxPAK3C"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pr_qst = pd.DataFrame([ ## dataset\n",
        "    {\"question\": \"What is the historical significance of the mosaic in the apse of the Church of Panagia Aggeloktisti and in what century was it created?\",\n",
        "     \"model_response\": \"\",\n",
        "     },\n",
        "    {\"question\": \"What architectural features distinguish the Church of Panagia Aggeloktisti and what periods of construction does it unite?\",\n",
        "     \"model_response\": \"\",\n",
        "     },\n",
        "    {\"question\": \"What does the name Aggeloktisti mean and where does it come from?\",\n",
        "     \"model_response\": \"\",\n",
        "     },\n",
        "    {\"question\": \"What chapels were added to the church in the Middle Ages and what was their purpose?\",\n",
        "     \"model_response\": \"\",\n",
        "     },\n",
        "    {\"question\": \"How does the Church of Panagia Aggeloktisti demonstrate the cultural and artistic links between different regions of the Mediterranean?\",\n",
        "     \"model_response\": \"\",\n",
        "     },\n",
        "    {\"question\": \"What arguments are put forward for the inclusion of the Church of Panagia Aggeloktisti in the UNESCO World Heritage List under criterion (i)?\",\n",
        "     \"model_response\": \"\",\n",
        "     },\n",
        "    {\"question\": \"What conservation measures have been taken since 1952 to preserve the mosaics and other elements of the church?\",\n",
        "     \"model_response\": \"\",\n",
        "     },\n",
        "    {\"question\": \"How does the church reflect the religious and spiritual life of the local community over the centuries?\",\n",
        "     \"model_response\": \"\",\n",
        "    },\n",
        "    {\"question\": \"How does the church of Panagia Aggeloktisti maintain its authenticity and integrity despite the passing centuries and the restorations carried out?\",\n",
        "     \"model_response\": \"\",\n",
        "    }\n",
        "])"
      ],
      "metadata": {
        "id": "22Sxz8Ci6uWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# llama = \"openlm-research/open_llama_3b\"\n",
        "# llama_model = AutoModelForCausalLM.from_pretrained(\n",
        "#     llama,\n",
        "#     torch_dtype=torch.float16, ## gpu support\n",
        "#     device_map=\"auto\",\n",
        "#     )\n",
        "# llama_tokenizer = AutoTokenizer.from_pretrained(llama)\n",
        "\n",
        "# qwen\n",
        "# !pip install transformers_stream_generator\n",
        "# !pip install hf_xet # http stuff\n",
        "# !pip install tiktoken\n",
        "# qwen = \"Qwen/Qwen-1_8B-Chat\"\n",
        "# qwen_model = AutoModelForCausalLM.from_pretrained(\n",
        "#     qwen,\n",
        "#     torch_dtype=torch.float16, ## gpu support\n",
        "#     device_map=\"auto\",\n",
        "#     trust_remote_code=True ## fix api issues\n",
        "#     )\n",
        "# qwen_tokenizer = AutoTokenizer.from_pretrained(qwen)\n",
        "\n",
        "\n",
        "# # deepseek\n",
        "# deepseek = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
        "# deepseek_model = AutoModelForCausalLM.from_pretrained(\n",
        "#     deepseek,\n",
        "#     torch_dtype=torch.float16, ## gpu support\n",
        "#     device_map=\"auto\",\n",
        "#     )\n",
        "# deepseek_tokenizer = AutoTokenizer.from_pretrained(deepseek)\n",
        "\n",
        "\n",
        "# falcon\n",
        "falcon = \"tiiuae/falcon-rw-1b\"\n",
        "falcon_model = AutoModelForCausalLM.from_pretrained(\n",
        "    falcon,\n",
        "    torch_dtype=torch.float16, ## gpu support\n",
        "    device_map=\"auto\",\n",
        "    offload_folder=\"offload\" ## memory issue\n",
        "    )\n",
        "falcon_tokenizer = AutoTokenizer.from_pretrained(falcon)\n",
        ""
      ],
      "metadata": {
        "id": "t4u6gwfU0oRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen(question, model, tokenizer):\n",
        "    prompt = f\"Question: {question}\\nAnswer:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=500,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=False, ## variety, turn off for now\n",
        "        top_p=0.95,\n",
        "        temperature=0.7, ## temp\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = answer.split(\"Answer:\")[-1].strip()\n",
        "    return answer\n",
        "\n",
        "\n",
        "# outputs\n",
        "llama_responses = [] # 1 minute 17 seconds runtime (3b model)\n",
        "falcon_responses = [] # 45 seconds (1b model)\n",
        "qwen_responses = []\n",
        "deepseek_responses = []\n",
        "\n",
        "# dataframes\n",
        "llama_dataframe = pd.DataFrame()\n",
        "falcon_dataframe = pd.DataFrame()\n",
        "qwen_dataframe = pd.DataFrame()\n",
        "deepseek_dataframe = pd.DataFrame()\n",
        "\n",
        "def output(model, tokenizer, responses, dataframe):\n",
        "    for question in pr_qst[\"question\"]:\n",
        "        resp = gen(question, model, tokenizer)\n",
        "        responses.append(resp)\n",
        "    dataframe = pr_qst.copy()\n",
        "    dataframe[\"model_response\"] = responses\n",
        "    return dataframe\n",
        "\n",
        "## llama\n",
        "# output(llama_model, llama_tokenizer, llama_responses, llama_dataframe)\n",
        "\n",
        "## falcon\n",
        "output(falcon_model, falcon_tokenizer, falcon_responses, falcon_dataframe)\n",
        "\n",
        "## qwen\n",
        "# output(qwen, qwen_tokenizer, qwen_responses, qwen_dataframe)\n",
        "\n",
        "## deepseek\n",
        "# output(deepseek, deepseek_tokenizer, deepseek_responses, deepseek_dataframe)"
      ],
      "metadata": {
        "id": "4qiunW1V854I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}