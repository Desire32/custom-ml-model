{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHzsHa5IdVkqUeKgYdYIhd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Hpi7YR-CfLDr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a3e20d8-b5fe-4838-9a27-db26831810b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nbimporter\n",
            "  Downloading nbimporter-0.3.4-py3-none-any.whl.metadata (252 bytes)\n",
            "Downloading nbimporter-0.3.4-py3-none-any.whl (4.9 kB)\n",
            "Installing collected packages: nbimporter\n",
            "Successfully installed nbimporter-0.3.4\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "!pip install nbimporter\n",
        "import nbimporter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# llama\n",
        "llama = \"openlm-research/open_llama_3b\"\n",
        "llama_model = AutoModelForCausalLM.from_pretrained(\n",
        "    llama,\n",
        "    torch_dtype=torch.float16, ## gpu support\n",
        "    device_map=\"auto\",\n",
        "    )\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained(llama)"
      ],
      "metadata": {
        "id": "12wW7Luh3y10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# falcon\n",
        "falcon = \"tiiuae/falcon-rw-1b\"\n",
        "falcon_model = AutoModelForCausalLM.from_pretrained(\n",
        "    falcon,\n",
        "    torch_dtype=torch.float16, ## gpu support\n",
        "    device_map=\"auto\",\n",
        "    )\n",
        "falcon_tokenizer = AutoTokenizer.from_pretrained(falcon)"
      ],
      "metadata": {
        "id": "q4wMJayl1_7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# qwen\n",
        "!pip install transformers_stream_generator\n",
        "!pip install hf_xet # http stuff\n",
        "!pip install tiktoken\n",
        "qwen = \"Qwen/Qwen-1_8B-Chat\"\n",
        "qwen_model = AutoModelForCausalLM.from_pretrained(\n",
        "    qwen,\n",
        "    torch_dtype=torch.float16, ## gpu support\n",
        "    device_map=\"auto\",\n",
        "    )\n",
        "qwen_tokenizer = AutoTokenizer.from_pretrained(qwen)"
      ],
      "metadata": {
        "id": "cYnf4y9u2lAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deepseek\n",
        "deepseek = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
        "deepseek_model = AutoModelForCausalLM.from_pretrained(\n",
        "    deepseek,\n",
        "    torch_dtype=torch.float16, ## gpu support\n",
        "    device_map=\"auto\",\n",
        "    )\n",
        "deepseek_tokenizer = AutoTokenizer.from_pretrained(deepseek)"
      ],
      "metadata": {
        "id": "nH4flTl53kYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M1KJ2ZZxt2xg"
      }
    }
  ]
}