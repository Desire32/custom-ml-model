{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN541jqB0RDHtWi2uGxgB+M"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization of libraries"
      ],
      "metadata": {
        "id": "zsa7qxCTxJoc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZCt5FxPAK3C"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Our predefined dataset with questions we will load into our models"
      ],
      "metadata": {
        "id": "Jky7blCwxGWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pr_qst = pd.DataFrame([ ## dataset\n",
        "    {\"question\": \"What is the historical significance of the mosaic in the apse of the Church of Panagia Aggeloktisti and in what century was it created?\",\n",
        "     \"model_response\": \"\",\n",
        "     },\n",
        "    {\"question\": \"What architectural features distinguish the Church of Panagia Aggeloktisti and what periods of construction does it unite?\",\n",
        "     \"model_response\": \"\",\n",
        "     },\n",
        "    {\"question\": \"What does the name Aggeloktisti mean and where does it come from?\",\n",
        "     \"model_response\": \"\",\n",
        "     },\n",
        "    {\"question\": \"What chapels were added to the church in the Middle Ages and what was their purpose?\",\n",
        "     \"model_response\": \"\",\n",
        "     },\n",
        "    {\"question\": \"How does the Church of Panagia Aggeloktisti demonstrate the cultural and artistic links between different regions of the Mediterranean?\",\n",
        "     \"model_response\": \"\",\n",
        "     },\n",
        "    {\"question\": \"What arguments are put forward for the inclusion of the Church of Panagia Aggeloktisti in the UNESCO World Heritage List under criterion (i)?\",\n",
        "     \"model_response\": \"\",\n",
        "     },\n",
        "    {\"question\": \"What conservation measures have been taken since 1952 to preserve the mosaics and other elements of the church?\",\n",
        "     \"model_response\": \"\",\n",
        "     },\n",
        "    {\"question\": \"How does the church reflect the religious and spiritual life of the local community over the centuries?\",\n",
        "     \"model_response\": \"\",\n",
        "    },\n",
        "    {\"question\": \"How does the church of Panagia Aggeloktisti maintain its authenticity and integrity despite the passing centuries and the restorations carried out?\",\n",
        "     \"model_response\": \"\",\n",
        "    }\n",
        "])"
      ],
      "metadata": {
        "id": "22Sxz8Ci6uWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models init"
      ],
      "metadata": {
        "id": "Lpe5C-JixX1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## llama\n",
        "\n",
        "# llama = \"openlm-research/open_llama_3b\"\n",
        "# llama_model = AutoModelForCausalLM.from_pretrained(\n",
        "#     llama,\n",
        "#     torch_dtype=torch.float16, ## gpu support\n",
        "#     device_map=\"auto\",\n",
        "#     )\n",
        "# llama_tokenizer = AutoTokenizer.from_pretrained(llama)\n",
        "\n",
        "\n",
        "# tiny-llama\n",
        "tiny = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tiny_model = AutoModelForCausalLM.from_pretrained(\n",
        "    tiny,\n",
        "    torch_dtype=torch.float16, ## gpu support\n",
        "    device_map=\"auto\",\n",
        "    )\n",
        "tiny_tokenizer = AutoTokenizer.from_pretrained(tiny)\n",
        "\n",
        "\n",
        "## phi\n",
        "# phi = \"microsoft/phi-2\"\n",
        "# phi_model = AutoModelForCausalLM.from_pretrained(\n",
        "#     phi,\n",
        "#     torch_dtype=torch.float16, ## gpu support\n",
        "#     device_map=\"auto\",\n",
        "#     trust_remote_code=True ## fix api issues\n",
        "#     )\n",
        "# phi_tokenizer = AutoTokenizer.from_pretrained(phi)\n",
        "\n",
        "# falcon\n",
        "# falcon = \"tiiuae/falcon-rw-1b\"\n",
        "# falcon_model = AutoModelForCausalLM.from_pretrained(\n",
        "#     falcon,\n",
        "#     torch_dtype=torch.float16, ## gpu support\n",
        "#     device_map=\"auto\",\n",
        "#     offload_folder=\"offload\" ## memory issue\n",
        "#     )\n",
        "# falcon_tokenizer = AutoTokenizer.from_pretrained(falcon)\n"
      ],
      "metadata": {
        "id": "t4u6gwfU0oRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate function to talk with LLM, responses to store output, output function to store in a list, after that save it in .csv file"
      ],
      "metadata": {
        "id": "du5ec8KGxb99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen(question, model, tokenizer):\n",
        "    prompt = f\"Question: {question}\\nAnswer:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=100,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=False, ## variety, turn off for now\n",
        "        top_p=0.95,\n",
        "        temperature=0.7, ## temp\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = answer.split(\"Answer:\")[-1].strip()\n",
        "    return answer\n",
        "\n",
        "# outputs\n",
        "llama_responses = [] # 1 minute 17 seconds runtime (3b model)\n",
        "falcon_responses = [] # 45 seconds (1b model)\n",
        "phi_responses = [] # 20 seconds -> 100 max_length (2b? model)\n",
        "tiny_responses = [] # 17 seconds -> 100 max_length (1b model)\n",
        "\n",
        "# dataframes\n",
        "llama_dataframe = pd.DataFrame()\n",
        "falcon_dataframe = pd.DataFrame()\n",
        "phi_dataframe = pd.DataFrame()\n",
        "tiny_dataframe = pd.DataFrame()\n",
        "\n",
        "def output(model, tokenizer, responses, dataframe):\n",
        "    for question in pr_qst[\"question\"]:\n",
        "        resp = gen(question, model, tokenizer)\n",
        "        responses.append(resp)\n",
        "    dataframe = pr_qst.copy()\n",
        "    dataframe[\"model_response\"] = responses\n",
        "    return dataframe\n",
        "\n",
        "## llama\n",
        "# llama_result = output(llama_model, llama_tokenizer, llama_responses, llama_dataframe)\n",
        "# llama_result.to_csv(\"llama_results.csv\")\n",
        "\n",
        "## falcon\n",
        "# falcon_result = output(falcon_model, falcon_tokenizer, falcon_responses, falcon_dataframe)\n",
        "# falcon_result.to_csv(\"falcon_results.csv\")\n",
        "\n",
        "## phi\n",
        "# phi_result = output(phi_model, phi_tokenizer, phi_responses, phi_dataframe)\n",
        "# phi_result.to_csv(\"phi_results.csv\")\n",
        "\n",
        "## tiny llama\n",
        "result = output(tiny_model, tiny_tokenizer, tiny_responses, tiny_dataframe)\n",
        "result.to_csv(\"tiny_llama_results.csv\")\n"
      ],
      "metadata": {
        "id": "4qiunW1V854I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E991l6r8i0Pw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}